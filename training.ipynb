{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b2562e",
   "metadata": {},
   "source": [
    "**Plant Disease Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d26d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Older but sometimes useful\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978c1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "dataset_dir = 'plants/plants'\n",
    "\n",
    "# Choose which image type to train on for THIS model instance\n",
    "# You will typically train a separate model for each type ('color', 'grayscale', 'segmented')\n",
    "image_type_to_train = 'color' # <-- CHANGE THIS for 'grayscale' or 'segmented'\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train', image_type_to_train)\n",
    "test_dir = os.path.join(dataset_dir, 'test', image_type_to_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d9f98",
   "metadata": {},
   "source": [
    "Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8912084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "IMG_WIDTH = 224 # Standard size for ResNet50\n",
    "IMG_HEIGHT = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS_PHASE1 = 10 # Fewer epochs for training only top layers\n",
    "EPOCHS_PHASE2 = 20 # More epochs for fine-tuning\n",
    "LEARNING_RATE_PHASE1 = 0.001\n",
    "LEARNING_RATE_PHASE2 = 0.0001 # Much smaller learning rate for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beb834a",
   "metadata": {},
   "source": [
    "GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02eebb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for GPU availability...\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of GPUs available: 1\n",
      "Using distribution strategy: MirroredStrategy\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for GPU availability...\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Use MirroredStrategy for potential single or multi-GPU training\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f\"Number of GPUs available: {len(gpus)}\")\n",
    "        print(\"Using distribution strategy: MirroredStrategy\")  # Updated this line\n",
    "        # You can configure GPU memory growth if needed\n",
    "        # for gpu in gpus:\n",
    "        #     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error initializing GPU: {e}\")\n",
    "        print(\"Falling back to CPU.\")\n",
    "        strategy = tf.distribute.get_strategy()  # Default strategy (CPU)\n",
    "else:\n",
    "    print(\"No GPU devices found. Using CPU.\")\n",
    "    strategy = tf.distribute.get_strategy()  # Default strategy (CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2088f9b4",
   "metadata": {},
   "source": [
    "Data Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "172b3d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 'color'...\n",
      "Found 43429 files belonging to 38 classes.\n",
      "Found 10876 files belonging to 38 classes.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading data for '{image_type_to_train}'...\")\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical', # Use 'categorical' for one-hot encoding, 'int' for sparse labels\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    interpolation='nearest',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    interpolation='nearest',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # No need to shuffle test data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8013d9",
   "metadata": {},
   "source": [
    "Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e71c0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38 classes: ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy', 'Cherry_(including_sour)___Powdery_mildew', 'Cherry_(including_sour)___healthy', 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot', 'Corn_(maize)___Common_rust_', 'Corn_(maize)___Northern_Leaf_Blight', 'Corn_(maize)___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', 'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy', 'Raspberry___healthy', 'Soybean___healthy', 'Squash___Powdery_mildew', 'Strawberry___Leaf_scorch', 'Strawberry___healthy', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
     ]
    }
   ],
   "source": [
    "# Get class names\n",
    "class_names = train_ds.class_names\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(f\"Found {NUM_CLASSES} classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785967da",
   "metadata": {},
   "source": [
    "Data Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22004bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformFullIntV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomGetKeyCounter cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting AdjustContrastv2 cause Input \"contrast_factor\" of op 'AdjustContrastv2' expected to be loop invariant.\n"
     ]
    }
   ],
   "source": [
    "# --- Data Augmentation ---\n",
    "# Using Keras Preprocessing Layers (recommended in TF 2.x)\n",
    "data_augmentation = keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "  tf.keras.layers.RandomZoom(0.2),\n",
    "  tf.keras.layers.RandomContrast(0.2),\n",
    "  # Add more augmentation layers as needed\n",
    "])\n",
    "\n",
    "# Apply data augmentation only to the training dataset\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "# Cache and prefetch data for performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ebeffd",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4126d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Model within Distribution Strategy Scope ---\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "\n",
      "--- Compiling Model (Phase 1: Training Top Layers) ---\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 224, 224, 3)      0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 224, 224, 3)      0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 38)                77862     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,665,574\n",
      "Trainable params: 77,862\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Building Model within Distribution Strategy Scope ---\")\n",
    "with strategy.scope():\n",
    "    # Load the pre-trained ResNet50 model\n",
    "    # Use weights='imagenet' for pre-trained weights\n",
    "    # include_top=False removes the final classification layer\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), # ResNet50 expects 3 color channels\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "\n",
    "    # Freeze the base model layers initially\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create the model architecture\n",
    "    inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    # Preprocessing: ResNet models expect input normalized in a specific way (ImageNet preprocessing)\n",
    "    # Using the built-in preprocessing layer is convenient\n",
    "    # This layer should be part of the model and will run on the GPU\n",
    "    x = tf.keras.applications.resnet50.preprocess_input(inputs)\n",
    "\n",
    "    # Pass the inputs through the base model\n",
    "    # Set training=False when using base_model as a fixed feature extractor in Phase 1\n",
    "    x = base_model(x, training=False)\n",
    "\n",
    "    # Add the new classification layers\n",
    "    x = GlobalAveragePooling2D()(x) # Reduces spatial dimensions\n",
    "    x = Dropout(0.5)(x) # Add dropout for regularization\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax')(x) # Final layer with 38 units and softmax activation\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "    # --- Compile the Model (Phase 1) ---\n",
    "    print(\"\\n--- Compiling Model (Phase 1: Training Top Layers) ---\")\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE_PHASE1),\n",
    "                  loss='categorical_crossentropy', # Matches label_mode='categorical'\n",
    "                  metrics=['accuracy',\n",
    "                            tf.keras.metrics.Precision(),\n",
    "                            tf.keras.metrics.Recall()])\n",
    "\n",
    "model.summary() # Print model architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143fe033",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8030be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training Phase 1 ---\n",
      "Epoch 1/10\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.6141 - accuracy: 0.8236 - precision: 0.9169 - recall: 0.7507\n",
      "Epoch 1: val_accuracy improved from -inf to 0.91596, saving model to best_model_color_phase1.h5\n",
      "1358/1358 [==============================] - 323s 233ms/step - loss: 0.6141 - accuracy: 0.8236 - precision: 0.9169 - recall: 0.7507 - val_loss: 0.2582 - val_accuracy: 0.9160 - val_precision: 0.9386 - val_recall: 0.8949\n",
      "Epoch 2/10\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.9158 - precision: 0.9395 - recall: 0.8934\n",
      "Epoch 2: val_accuracy improved from 0.91596 to 0.93371, saving model to best_model_color_phase1.h5\n",
      "1358/1358 [==============================] - 233s 171ms/step - loss: 0.2703 - accuracy: 0.9158 - precision: 0.9395 - recall: 0.8934 - val_loss: 0.2045 - val_accuracy: 0.9337 - val_precision: 0.9449 - val_recall: 0.9231\n",
      "Epoch 3/10\n",
      "1357/1358 [============================>.] - ETA: 0s - loss: 0.2325 - accuracy: 0.9256 - precision: 0.9437 - recall: 0.9112\n",
      "Epoch 3: val_accuracy did not improve from 0.93371\n",
      "1358/1358 [==============================] - 197s 145ms/step - loss: 0.2325 - accuracy: 0.9256 - precision: 0.9437 - recall: 0.9112 - val_loss: 0.2128 - val_accuracy: 0.9321 - val_precision: 0.9415 - val_recall: 0.9240\n",
      "Epoch 4/10\n",
      "1357/1358 [============================>.] - ETA: 0s - loss: 0.2165 - accuracy: 0.9292 - precision: 0.9428 - recall: 0.9178\n",
      "Epoch 4: val_accuracy improved from 0.93371 to 0.93757, saving model to best_model_color_phase1.h5\n",
      "1358/1358 [==============================] - 189s 139ms/step - loss: 0.2165 - accuracy: 0.9292 - precision: 0.9429 - recall: 0.9178 - val_loss: 0.1909 - val_accuracy: 0.9376 - val_precision: 0.9456 - val_recall: 0.9310\n",
      "Epoch 5/10\n",
      "1357/1358 [============================>.] - ETA: 0s - loss: 0.2009 - accuracy: 0.9331 - precision: 0.9446 - recall: 0.9235\n",
      "Epoch 5: val_accuracy did not improve from 0.93757\n",
      "1358/1358 [==============================] - 167s 123ms/step - loss: 0.2009 - accuracy: 0.9331 - precision: 0.9446 - recall: 0.9236 - val_loss: 0.2270 - val_accuracy: 0.9293 - val_precision: 0.9380 - val_recall: 0.9233\n",
      "Epoch 6/10\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.1932 - accuracy: 0.9369 - precision: 0.9472 - recall: 0.9284\n",
      "Epoch 6: val_accuracy did not improve from 0.93757\n",
      "1358/1358 [==============================] - 220s 162ms/step - loss: 0.1932 - accuracy: 0.9369 - precision: 0.9472 - recall: 0.9284 - val_loss: 0.2074 - val_accuracy: 0.9342 - val_precision: 0.9407 - val_recall: 0.9298\n",
      "Epoch 7/10\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9382 - precision: 0.9468 - recall: 0.9313\n",
      "Epoch 7: val_accuracy improved from 0.93757 to 0.93968, saving model to best_model_color_phase1.h5\n",
      "1358/1358 [==============================] - 217s 160ms/step - loss: 0.1917 - accuracy: 0.9382 - precision: 0.9468 - recall: 0.9313 - val_loss: 0.1923 - val_accuracy: 0.9397 - val_precision: 0.9464 - val_recall: 0.9359\n",
      "Epoch 8/10\n",
      "1357/1358 [============================>.] - ETA: 0s - loss: 0.1743 - accuracy: 0.9436 - precision: 0.9524 - recall: 0.9368\n",
      "Epoch 8: val_accuracy did not improve from 0.93968\n",
      "1358/1358 [==============================] - 221s 162ms/step - loss: 0.1743 - accuracy: 0.9436 - precision: 0.9524 - recall: 0.9368 - val_loss: 0.2199 - val_accuracy: 0.9344 - val_precision: 0.9398 - val_recall: 0.9309\n",
      "Epoch 9/10\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9421 - precision: 0.9504 - recall: 0.9361\n",
      "Epoch 9: val_accuracy improved from 0.93968 to 0.94143, saving model to best_model_color_phase1.h5\n",
      "1358/1358 [==============================] - 219s 161ms/step - loss: 0.1801 - accuracy: 0.9421 - precision: 0.9504 - recall: 0.9361 - val_loss: 0.1888 - val_accuracy: 0.9414 - val_precision: 0.9465 - val_recall: 0.9378\n",
      "Epoch 10/10\n",
      "1357/1358 [============================>.] - ETA: 0s - loss: 0.1826 - accuracy: 0.9424 - precision: 0.9493 - recall: 0.9369\n",
      "Epoch 10: val_accuracy improved from 0.94143 to 0.94612, saving model to best_model_color_phase1.h5\n",
      "1358/1358 [==============================] - 190s 139ms/step - loss: 0.1826 - accuracy: 0.9424 - precision: 0.9493 - recall: 0.9369 - val_loss: 0.1760 - val_accuracy: 0.9461 - val_precision: 0.9513 - val_recall: 0.9433\n"
     ]
    }
   ],
   "source": [
    "# --- Training Phase 1 ---\n",
    "print(\"\\n--- Starting Training Phase 1 ---\")\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint_filepath_phase1 = f'best_model_{image_type_to_train}_phase1.h5'\n",
    "model_checkpoint_callback_phase1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_phase1,\n",
    "    save_best_only=True,        # Save only the model with the best validation accuracy\n",
    "    monitor='val_accuracy',      # Metric to monitor\n",
    "    mode='max',                 # We want to maximize validation accuracy\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback_phase1 = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',      # Monitor validation loss\n",
    "    patience=5,              # Stop if val_loss doesn't improve for 5 epochs\n",
    "    restore_best_weights=True # Restore weights from the best epoch\n",
    ")\n",
    "\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    validation_data=test_ds, # Use the test set for validation\n",
    "    callbacks=[model_checkpoint_callback_phase1, early_stopping_callback_phase1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982302a",
   "metadata": {},
   "source": [
    "Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b59814d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading best model from Phase 1: best_model_color_phase1.h5\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from Phase 1 (ensures we start fine-tuning from the best point)\n",
    "print(f\"\\nLoading best model from Phase 1: {checkpoint_filepath_phase1}\")\n",
    "# Loading weights also happens within the strategy's context usually, but explicit load is fine\n",
    "model.load_weights(checkpoint_filepath_phase1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c56dd",
   "metadata": {},
   "source": [
    "Fine Tuning the Model Phase - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac32e2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up Fine-tuning (Phase 2) ---\n",
      "\n",
      "--- Recompiling Model (Phase 2: Fine-tuning) ---\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 224, 224, 3)      0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 224, 224, 3)      0         \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 2048)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 38)                77862     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,665,574\n",
      "Trainable params: 14,528,038\n",
      "Non-trainable params: 9,137,536\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# --- Fine-tuning Setup (Phase 2) ---\n",
    "print(\"\\n--- Setting up Fine-tuning (Phase 2) ---\")\n",
    "\n",
    "# Unfreeze the base model (or parts of it)\n",
    "base_model.trainable = True\n",
    "\n",
    "# Decide how many layers to fine-tune. It's common to fine-tune the later layers\n",
    "# Let's find the number of layers to unfreeze. ResNet50 has ~175 layers in the base model.\n",
    "# You can choose to unfreeze the last block or last few blocks.\n",
    "# Example: Unfreeze the last ~30 layers (adjust this number based on experimentation)\n",
    "# You can print base_model.summary() to see layer names and count\n",
    "fine_tune_from_layer = -30 # Fine-tune the last 30 layers\n",
    "\n",
    "# Freeze all layers except the last `fine_tune_from_layer`\n",
    "for layer in base_model.layers[:len(base_model.layers) + fine_tune_from_layer]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile the model with a much lower learning rate\n",
    "# Recompilation MUST happen inside the strategy scope if you modify trainable status\n",
    "with strategy.scope():\n",
    "    print(\"\\n--- Recompiling Model (Phase 2: Fine-tuning) ---\")\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE_PHASE2), # Use a very low learning rate\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy',\n",
    "                            tf.keras.metrics.Precision(),\n",
    "                            tf.keras.metrics.Recall()])\n",
    "\n",
    "model.summary() # Print model architecture again to see which layers are trainable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9d28b",
   "metadata": {},
   "source": [
    "Training the Model Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b62c0018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training Phase 2 (Fine-tuning) ---\n",
      "Epoch 1/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9402 - precision_1: 0.9520 - recall_1: 0.9308\n",
      "Epoch 1: val_accuracy improved from -inf to 0.93288, saving model to best_model_color_phase2.h5\n",
      "1358/1358 [==============================] - 241s 174ms/step - loss: 0.1847 - accuracy: 0.9402 - precision_1: 0.9520 - recall_1: 0.9308 - val_loss: 0.2188 - val_accuracy: 0.9329 - val_precision_1: 0.9370 - val_recall_1: 0.9294 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9687 - precision_1: 0.9729 - recall_1: 0.9638\n",
      "Epoch 2: val_accuracy improved from 0.93288 to 0.96331, saving model to best_model_color_phase2.h5\n",
      "1358/1358 [==============================] - 235s 173ms/step - loss: 0.0972 - accuracy: 0.9687 - precision_1: 0.9729 - recall_1: 0.9638 - val_loss: 0.1115 - val_accuracy: 0.9633 - val_precision_1: 0.9656 - val_recall_1: 0.9615 - lr: 1.0000e-04\n",
      "Epoch 3/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9760 - precision_1: 0.9788 - recall_1: 0.9736\n",
      "Epoch 3: val_accuracy improved from 0.96331 to 0.97913, saving model to best_model_color_phase2.h5\n",
      "1358/1358 [==============================] - 230s 169ms/step - loss: 0.0715 - accuracy: 0.9760 - precision_1: 0.9788 - recall_1: 0.9736 - val_loss: 0.0585 - val_accuracy: 0.9791 - val_precision_1: 0.9811 - val_recall_1: 0.9778 - lr: 1.0000e-04\n",
      "Epoch 4/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9813 - precision_1: 0.9832 - recall_1: 0.9796\n",
      "Epoch 4: val_accuracy did not improve from 0.97913\n",
      "1358/1358 [==============================] - 228s 168ms/step - loss: 0.0554 - accuracy: 0.9813 - precision_1: 0.9832 - recall_1: 0.9796 - val_loss: 0.0870 - val_accuracy: 0.9723 - val_precision_1: 0.9745 - val_recall_1: 0.9705 - lr: 1.0000e-04\n",
      "Epoch 5/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9844 - precision_1: 0.9860 - recall_1: 0.9835\n",
      "Epoch 5: val_accuracy did not improve from 0.97913\n",
      "1358/1358 [==============================] - 242s 178ms/step - loss: 0.0467 - accuracy: 0.9844 - precision_1: 0.9860 - recall_1: 0.9835 - val_loss: 0.0974 - val_accuracy: 0.9702 - val_precision_1: 0.9730 - val_recall_1: 0.9688 - lr: 1.0000e-04\n",
      "Epoch 6/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0394 - accuracy: 0.9872 - precision_1: 0.9883 - recall_1: 0.9864\n",
      "Epoch 6: val_accuracy improved from 0.97913 to 0.98235, saving model to best_model_color_phase2.h5\n",
      "1358/1358 [==============================] - 251s 184ms/step - loss: 0.0394 - accuracy: 0.9872 - precision_1: 0.9883 - recall_1: 0.9864 - val_loss: 0.0600 - val_accuracy: 0.9823 - val_precision_1: 0.9831 - val_recall_1: 0.9812 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9882 - precision_1: 0.9890 - recall_1: 0.9874\n",
      "Epoch 7: val_accuracy did not improve from 0.98235\n",
      "1358/1358 [==============================] - 258s 190ms/step - loss: 0.0364 - accuracy: 0.9882 - precision_1: 0.9890 - recall_1: 0.9874 - val_loss: 0.1013 - val_accuracy: 0.9724 - val_precision_1: 0.9739 - val_recall_1: 0.9717 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9903 - precision_1: 0.9910 - recall_1: 0.9898\n",
      "Epoch 8: val_accuracy did not improve from 0.98235\n",
      "1358/1358 [==============================] - 213s 157ms/step - loss: 0.0307 - accuracy: 0.9903 - precision_1: 0.9910 - recall_1: 0.9898 - val_loss: 0.0893 - val_accuracy: 0.9763 - val_precision_1: 0.9777 - val_recall_1: 0.9759 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9911 - precision_1: 0.9917 - recall_1: 0.9907\n",
      "Epoch 9: val_accuracy did not improve from 0.98235\n",
      "1358/1358 [==============================] - 200s 147ms/step - loss: 0.0266 - accuracy: 0.9911 - precision_1: 0.9917 - recall_1: 0.9907 - val_loss: 0.0867 - val_accuracy: 0.9784 - val_precision_1: 0.9795 - val_recall_1: 0.9775 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9907 - precision_1: 0.9912 - recall_1: 0.9903\n",
      "Epoch 10: val_accuracy did not improve from 0.98235\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "1358/1358 [==============================] - 200s 148ms/step - loss: 0.0284 - accuracy: 0.9907 - precision_1: 0.9912 - recall_1: 0.9903 - val_loss: 0.1190 - val_accuracy: 0.9690 - val_precision_1: 0.9705 - val_recall_1: 0.9676 - lr: 1.0000e-04\n",
      "Epoch 11/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0078 - accuracy: 0.9975 - precision_1: 0.9975 - recall_1: 0.9974\n",
      "Epoch 11: val_accuracy improved from 0.98235 to 0.98437, saving model to best_model_color_phase2.h5\n",
      "1358/1358 [==============================] - 203s 150ms/step - loss: 0.0078 - accuracy: 0.9975 - precision_1: 0.9975 - recall_1: 0.9974 - val_loss: 0.0609 - val_accuracy: 0.9844 - val_precision_1: 0.9848 - val_recall_1: 0.9839 - lr: 1.0000e-05\n",
      "Epoch 12/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990 - precision_1: 0.9991 - recall_1: 0.9990\n",
      "Epoch 12: val_accuracy improved from 0.98437 to 0.98510, saving model to best_model_color_phase2.h5\n",
      "1358/1358 [==============================] - 202s 149ms/step - loss: 0.0032 - accuracy: 0.9990 - precision_1: 0.9991 - recall_1: 0.9990 - val_loss: 0.0588 - val_accuracy: 0.9851 - val_precision_1: 0.9860 - val_recall_1: 0.9845 - lr: 1.0000e-05\n",
      "Epoch 13/20\n",
      "1358/1358 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 0.9995 - precision_1: 0.9996 - recall_1: 0.9995\n",
      "Epoch 13: val_accuracy did not improve from 0.98510\n",
      "1358/1358 [==============================] - 202s 149ms/step - loss: 0.0018 - accuracy: 0.9995 - precision_1: 0.9996 - recall_1: 0.9995 - val_loss: 0.0682 - val_accuracy: 0.9834 - val_precision_1: 0.9842 - val_recall_1: 0.9833 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Starting Training Phase 2 (Fine-tuning) ---\")\n",
    "\n",
    "# Define callbacks for Phase 2\n",
    "checkpoint_filepath_phase2 = f'best_model_{image_type_to_train}_phase2.h5'\n",
    "model_checkpoint_callback_phase2 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath_phase2,\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping_callback_phase2 = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', # or 'val_accuracy'\n",
    "    patience=10,         # More patience for fine-tuning\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1, # Reduce learning rate by a factor of 10\n",
    "    patience=7, # If val_loss doesn't improve for 7 epochs\n",
    "    min_lr=0.000001, # Minimum learning rate\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    validation_data=test_ds,\n",
    "    callbacks=[model_checkpoint_callback_phase2, early_stopping_callback_phase2, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe881c",
   "metadata": {},
   "source": [
    "Phase 2 best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61558a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading best model after Fine-tuning: best_model_color_phase2.h5\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from Phase 2 for final evaluation\n",
    "print(f\"\\nLoading best model after Fine-tuning: {checkpoint_filepath_phase2}\")\n",
    "# Loading the final model for evaluation\n",
    "best_model = tf.keras.models.load_model(checkpoint_filepath_phase2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307e017",
   "metadata": {},
   "source": [
    "Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70fd3234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating the Final Model on the Test Set ---\n",
      "340/340 [==============================] - 34s 98ms/step - loss: 0.0588 - accuracy: 0.9851 - precision_1: 0.9860 - recall_1: 0.9845\n",
      "Test Loss: 0.0588\n",
      "Test Accuracy: 0.9851\n",
      "Test Precision: 0.9860\n",
      "Test Recall: 0.9845\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Evaluating the Final Model on the Test Set ---\")\n",
    "# Evaluation also happens using the strategized model\n",
    "loss, accuracy, precision, recall = best_model.evaluate(test_ds)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0319a2",
   "metadata": {},
   "source": [
    "Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03cc588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating the Final Model on the Test Set ---\n",
      "340/340 [==============================] - 35s 102ms/step - loss: 0.0588 - accuracy: 0.9851 - precision_1: 0.9860 - recall_1: 0.9845\n",
      "Test Loss: 0.0588\n",
      "Test Accuracy: 0.9851\n",
      "Test Precision: 0.9860\n",
      "Test Recall: 0.9845\n",
      "\n",
      "--- Generating Classification Report and Confusion Matrix ---\n",
      "340/340 [==============================] - 34s 94ms/step\n",
      "\n",
      "Classification Report:\n",
      "                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                Apple___Apple_scab       0.98      1.00      0.99       126\n",
      "                                 Apple___Black_rot       0.98      1.00      0.99       125\n",
      "                          Apple___Cedar_apple_rust       1.00      0.98      0.99        55\n",
      "                                   Apple___healthy       0.99      1.00      1.00       329\n",
      "                               Blueberry___healthy       0.99      1.00      1.00       301\n",
      "          Cherry_(including_sour)___Powdery_mildew       1.00      1.00      1.00       211\n",
      "                 Cherry_(including_sour)___healthy       1.00      1.00      1.00       171\n",
      "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot       0.87      0.96      0.91       103\n",
      "                       Corn_(maize)___Common_rust_       1.00      1.00      1.00       239\n",
      "               Corn_(maize)___Northern_Leaf_Blight       0.98      0.92      0.95       197\n",
      "                            Corn_(maize)___healthy       1.00      1.00      1.00       233\n",
      "                                 Grape___Black_rot       1.00      1.00      1.00       236\n",
      "                      Grape___Esca_(Black_Measles)       1.00      1.00      1.00       277\n",
      "        Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       1.00      1.00      1.00       216\n",
      "                                   Grape___healthy       1.00      1.00      1.00        85\n",
      "          Orange___Haunglongbing_(Citrus_greening)       1.00      1.00      1.00      1102\n",
      "                            Peach___Bacterial_spot       1.00      1.00      1.00       460\n",
      "                                   Peach___healthy       0.99      0.99      0.99        72\n",
      "                     Pepper,_bell___Bacterial_spot       1.00      1.00      1.00       200\n",
      "                            Pepper,_bell___healthy       1.00      0.98      0.99       296\n",
      "                             Potato___Early_blight       0.99      1.00      1.00       200\n",
      "                              Potato___Late_blight       0.97      0.96      0.97       200\n",
      "                                  Potato___healthy       0.97      0.90      0.93        31\n",
      "                               Raspberry___healthy       1.00      1.00      1.00        75\n",
      "                                 Soybean___healthy       0.98      1.00      0.99      1018\n",
      "                           Squash___Powdery_mildew       1.00      1.00      1.00       367\n",
      "                          Strawberry___Leaf_scorch       1.00      1.00      1.00       222\n",
      "                              Strawberry___healthy       1.00      1.00      1.00        92\n",
      "                           Tomato___Bacterial_spot       0.98      0.98      0.98       426\n",
      "                             Tomato___Early_blight       0.99      0.86      0.92       200\n",
      "                              Tomato___Late_blight       0.97      0.98      0.98       382\n",
      "                                Tomato___Leaf_Mold       0.99      0.95      0.97       191\n",
      "                       Tomato___Septoria_leaf_spot       0.97      0.99      0.98       355\n",
      "     Tomato___Spider_mites Two-spotted_spider_mite       0.97      0.90      0.94       336\n",
      "                              Tomato___Target_Spot       0.88      0.94      0.91       281\n",
      "            Tomato___Tomato_Yellow_Leaf_Curl_Virus       1.00      0.99      0.99      1072\n",
      "                      Tomato___Tomato_mosaic_virus       0.99      1.00      0.99        75\n",
      "                                  Tomato___healthy       0.93      1.00      0.96       319\n",
      "\n",
      "                                          accuracy                           0.99     10876\n",
      "                                         macro avg       0.98      0.98      0.98     10876\n",
      "                                      weighted avg       0.99      0.99      0.99     10876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"\\n--- Generating Classification Report and Confusion Matrix ---\")\n",
    "\n",
    "# # Get true labels and predictions\n",
    "# y_true = []\n",
    "# y_pred_probs = []\n",
    "\n",
    "# # Use predict on the dataset. This will also utilize the GPU via the strategy\n",
    "# # Predict might require converting dataset to numpy or iterating\n",
    "# # A common way to get predictions for metrics:\n",
    "# test_images = np.concatenate([x for x, y in test_ds], axis=0)\n",
    "# test_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "\n",
    "# y_true = np.argmax(test_labels, axis=1)\n",
    "# y_pred_probs = best_model.predict(test_images) # Prediction happens on GPU\n",
    "# y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "print(\"\\n--- Evaluating the Final Model on the Test Set ---\")\n",
    "# Evaluation also happens using the strategized model\n",
    "# Evaluate using the dataset directly\n",
    "loss, accuracy, precision, recall = best_model.evaluate(test_ds)\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "\n",
    "# --- Detailed Metrics (Optional but Recommended) ---\n",
    "print(\"\\n--- Generating Classification Report and Confusion Matrix ---\")\n",
    "\n",
    "# Get true labels and predictions using the dataset directly\n",
    "# Predict using the dataset. This will also utilize the GPU via the strategy.\n",
    "# The predict method on a dataset returns predictions for each batch.\n",
    "# We need to collect these predictions and the corresponding true labels.\n",
    "\n",
    "y_pred_probs = best_model.predict(test_ds) # Prediction happens on GPU, returns batched predictions\n",
    "y_pred = np.argmax(y_pred_probs, axis=1) # Convert probabilities to predicted class indices\n",
    "\n",
    "# To get the true labels in the correct order, iterate through the dataset again\n",
    "# Since test_ds has shuffle=False, iterating through it will give batches\n",
    "# in the same order as predict() processed them.\n",
    "y_true = []\n",
    "for images, labels in test_ds:\n",
    "    # labels are already one-hot encoded, convert back to integer indices\n",
    "    y_true.extend(tf.argmax(labels, axis=1).numpy())\n",
    "\n",
    "y_true = np.array(y_true) # Convert list to numpy array for scikit-learn\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "# Ensure class_names are in the same order as the dataset indexed them\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd4ca6",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c356fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                Apple___Apple_scab       0.98      1.00      0.99       126\n",
      "                                 Apple___Black_rot       0.98      1.00      0.99       125\n",
      "                          Apple___Cedar_apple_rust       1.00      0.98      0.99        55\n",
      "                                   Apple___healthy       0.99      1.00      1.00       329\n",
      "                               Blueberry___healthy       0.99      1.00      1.00       301\n",
      "          Cherry_(including_sour)___Powdery_mildew       1.00      1.00      1.00       211\n",
      "                 Cherry_(including_sour)___healthy       1.00      1.00      1.00       171\n",
      "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot       0.87      0.96      0.91       103\n",
      "                       Corn_(maize)___Common_rust_       1.00      1.00      1.00       239\n",
      "               Corn_(maize)___Northern_Leaf_Blight       0.98      0.92      0.95       197\n",
      "                            Corn_(maize)___healthy       1.00      1.00      1.00       233\n",
      "                                 Grape___Black_rot       1.00      1.00      1.00       236\n",
      "                      Grape___Esca_(Black_Measles)       1.00      1.00      1.00       277\n",
      "        Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       1.00      1.00      1.00       216\n",
      "                                   Grape___healthy       1.00      1.00      1.00        85\n",
      "          Orange___Haunglongbing_(Citrus_greening)       1.00      1.00      1.00      1102\n",
      "                            Peach___Bacterial_spot       1.00      1.00      1.00       460\n",
      "                                   Peach___healthy       0.99      0.99      0.99        72\n",
      "                     Pepper,_bell___Bacterial_spot       1.00      1.00      1.00       200\n",
      "                            Pepper,_bell___healthy       1.00      0.98      0.99       296\n",
      "                             Potato___Early_blight       0.99      1.00      1.00       200\n",
      "                              Potato___Late_blight       0.97      0.96      0.97       200\n",
      "                                  Potato___healthy       0.97      0.90      0.93        31\n",
      "                               Raspberry___healthy       1.00      1.00      1.00        75\n",
      "                                 Soybean___healthy       0.98      1.00      0.99      1018\n",
      "                           Squash___Powdery_mildew       1.00      1.00      1.00       367\n",
      "                          Strawberry___Leaf_scorch       1.00      1.00      1.00       222\n",
      "                              Strawberry___healthy       1.00      1.00      1.00        92\n",
      "                           Tomato___Bacterial_spot       0.98      0.98      0.98       426\n",
      "                             Tomato___Early_blight       0.99      0.86      0.92       200\n",
      "                              Tomato___Late_blight       0.97      0.98      0.98       382\n",
      "                                Tomato___Leaf_Mold       0.99      0.95      0.97       191\n",
      "                       Tomato___Septoria_leaf_spot       0.97      0.99      0.98       355\n",
      "     Tomato___Spider_mites Two-spotted_spider_mite       0.97      0.90      0.94       336\n",
      "                              Tomato___Target_Spot       0.88      0.94      0.91       281\n",
      "            Tomato___Tomato_Yellow_Leaf_Curl_Virus       1.00      0.99      0.99      1072\n",
      "                      Tomato___Tomato_mosaic_virus       0.99      1.00      0.99        75\n",
      "                                  Tomato___healthy       0.93      1.00      0.96       319\n",
      "\n",
      "                                          accuracy                           0.99     10876\n",
      "                                         macro avg       0.98      0.98      0.98     10876\n",
      "                                      weighted avg       0.99      0.99      0.99     10876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "# Confusion Matrix (can be large for 38 classes, but useful)\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# You can also plot the confusion matrix if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a4fd3",
   "metadata": {},
   "source": [
    "Plotting training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e867e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for 'color' completed. Best model saved to best_model_color_phase2.h5\n",
      "Remember to repeat this process or adapt the code for 'grayscale' and 'segmented' data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_history(history_phase1, history_phase2):\n",
    "    acc1 = history_phase1.history['accuracy']\n",
    "    val_acc1 = history_phase1.history['val_accuracy']\n",
    "    loss1 = history_phase1.history['loss']\n",
    "    val_loss1 = history_phase1.history['val_loss']\n",
    "\n",
    "    acc2 = history_phase2.history['accuracy']\n",
    "    val_acc2 = history_phase2.history['val_accuracy']\n",
    "    loss2 = history_phase2.history['loss']\n",
    "    val_loss2 = history_phase2.history['val_loss']\n",
    "\n",
    "    # Combine histories\n",
    "    total_acc = acc1 + acc2\n",
    "    total_val_acc = val_acc1 + val_acc2\n",
    "    total_loss = loss1 + loss2\n",
    "    total_val_loss = val_loss1 + val_loss2\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(total_acc, label='Training Accuracy')\n",
    "    plt.plot(total_val_acc, label='Validation Accuracy')\n",
    "    plt.axvline(x=EPOCHS_PHASE1-1, color='r', linestyle='--', label='Phase 1 End')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(total_loss, label='Training Loss')\n",
    "    plt.plot(total_val_loss, label='Validation Loss')\n",
    "    plt.axvline(x=EPOCHS_PHASE1-1, color='r', linestyle='--', label='Phase 1 End')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    # Calculate y-axis limits based on actual data\n",
    "    max_loss = max(max(total_loss), max(total_val_loss)) if total_loss and total_val_loss else 1\n",
    "    plt.ylim([0, max_loss * 1.1])\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the plotting function if matplotlib is available\n",
    "# plot_history(history_phase1, history_phase2) # Uncomment to show plots\n",
    "\n",
    "print(f\"\\nTraining for '{image_type_to_train}' completed. Best model saved to {checkpoint_filepath_phase2}\")\n",
    "print(\"Remember to repeat this process or adapt the code for 'grayscale' and 'segmented' data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
